{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech corpus can be downloaded here: \n",
    "# http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of speech titles\n",
    "speech_dir = \"./president_speech_corpus/**/*.txt\" # set directory where speeches are contained here\n",
    "\n",
    "titles = []\n",
    "\n",
    "\n",
    "for file in sorted(glob.glob(speech_dir)):\n",
    "    title = open(file)\n",
    "    title = title.readline()\n",
    "    title = title[8:-3]\n",
    "    titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of president names\n",
    "\n",
    "pres_names = []\n",
    "\n",
    "for file in sorted(glob.glob(speech_dir)):\n",
    "    pres_names.append(file)\n",
    "\n",
    "\n",
    "pres_names2 = []\n",
    "\n",
    "for count in range(len(pres_names)):\n",
    "    pres_names2.append(pres_names[count].split('/')[3].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''list of speech dates - In the original txt files, there are 8 dates in the wrong place. You can identify the files\n",
    "using pd.to_datetime(speech_dates, errors = \"coerce\"), which will set those speech dates to NaT (Not a Time). I\n",
    "manually changed moved the dates in those files, since that was the simplest solution. May come back and write code\n",
    "to grab those dates\n",
    "'''\n",
    "\n",
    "speech_dates = []\n",
    "for file in sorted(glob.glob(speech_dir)):\n",
    "    date = open(file)\n",
    "    date.readline()\n",
    "    date = date.readline()\n",
    "    date = date[7:-3]\n",
    "    speech_dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of full speeches\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for file in sorted(glob.glob(speech_dir)):\n",
    "    speech = open(file)\n",
    "    speech = speech.read()\n",
    "    speech = re.sub(r'<.*>', '', speech)\n",
    "    speech = speech.replace('\\n', ' ')\n",
    "    speeches.append(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create speech dataframe (set to_datetime argument errors = \"coerce\" here to identify incorrectly dated files)\n",
    "# LIST ISSUE FILES HERE\n",
    "\n",
    "df = pd.DataFrame({'title' : titles, 'pres_name': pres_names2, 'speech' : speeches}, \n",
    "                  index = pd.to_datetime(speech_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Date filter\n",
    "\n",
    "df = df[df.index.year >= 1900]\n",
    "\n",
    "# filter press conferences and debates\n",
    "\n",
    "# df = df[-df.title.str.contains('press', case = False)]\n",
    "# df = df[-df.title.str.contains('debate', case = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare words for NLP (tokenize, punctuation removal, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "\n",
    "speeches = [sentence.lower() for sentence in df.speech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "punc_remove = [word_tokenizer.tokenize(word) for word in speeches]\n",
    "\n",
    "punc_remove = [' '.join(lst) for lst in punc_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stopwords - changing this list can have a dramatic effect on results in the LDA model because it uses word\n",
    "# counts, while NMF won't be affected much, since it uses TFIDF\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "extra_stopwords = ['people', 'united', 'states', 'america', 'american', \n",
    "                   'us', 'nation', 'great', 'one', 'would', 'great',\n",
    "                  'government', 'upon', 'must', 'every', 'many', 'made', \n",
    "                   'may', 'also', 'shall', 'think', 'mr', 'know', 'want', 'without',\n",
    "                  '000', 'years', 'year', 'believe', 'uh', 'going', 'make', 'americans', 'let',\n",
    "                  'men', 'public', 'man', 'today', 'whereas', 'part', 'federal', 'hereby', 'last', \n",
    "                   'present', 'within', 'dr', 'mrs', 'therefore', 'aforesaid', 'could', 'things', \n",
    "                   'much', 'get', 'say', 'applause'\n",
    "                  ]\n",
    "\n",
    "stopwords.update(extra_stopwords)\n",
    "\n",
    "# Additional stopwords that can impact results, but it can be argued that they are important words\n",
    "\n",
    "# more_stopwords = ['world', 'new', 'country', 'president', 'said', 'day', 'republic', 'general', 'law',\n",
    "#                  'laws', 'state', 'constitution', 'time', 'citizens', 'citizen', 'duty']\n",
    "\n",
    "# stopwords.update(more_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "# split and rejoin list of strings to drop stopwords and retokenize\n",
    "\n",
    "punc_remove = [' '.join(w for w in word.split() if w not in stopwords) for word in punc_remove]\n",
    "\n",
    "punc_remove = [word_tokenizer.tokenize(word) for word in punc_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary and bag of word corpus for LdaModel\n",
    "\n",
    "dictionary = corpora.Dictionary(punc_remove)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in punc_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 36.,  51.,  48.,  48.,  19., 120.,  66.,  75.,  57.,  65.]),\n",
       " array([693788. , 698019.1, 702250.2, 706481.3, 710712.4, 714943.5,\n",
       "        719174.6, 723405.7, 727636.8, 731867.9, 736099. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEKRJREFUeJzt3X+sJWV9x/H3R1ZQsXZ35XaDLHTXiG3AQKQ3lKqxVtqKYFxarUFp3QrJtqltsWplqU2waUxATf2RtiYbQJeUgBQ10IraLcWQ1oLuogILAis/ZLcLexX8nYrot3+coRzW/XXPnOu95z7vV3JzZp6ZOfN9zrP3s3NmzpmbqkKS1I6nzHcBkqSfLYNfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Jgl810AwGGHHVarVq2a7zIkaaJs2bLlG1U1NdvtFkTwr1q1is2bN893GZI0UZLcP8p2nuqRpMYY/JLUGINfkhpj8EtSYwx+SWrMfoM/ySVJdiW5bajtvUm+muSWJJ9MsnRo2XlJtiW5M8kr5qpwSdJoDuSI/6PAKbu1bQJeUFXHAXcB5wEkOQY4Azi22+Yfkxw0tmolSb3tN/ir6gbg4d3a/q2qHutmbwRWdtNrgCuq6odVdS+wDThxjPVKknoaxzn+s4BPd9NHAA8MLdvetUmSFohe39xN8k7gMeCyEbZdB6wDOOqoo/qUIc2pVes/NS/7ve+C0+Zlv1r8Rj7iT/KHwKuAM6uquuYdwJFDq63s2n5KVW2oqumqmp6amvWtJiRJIxop+JOcArwDeHVV/WBo0TXAGUkOSbIaOBr4Qv8yJUnjst9TPUkuB14GHJZkO3A+g0/xHAJsSgJwY1X9cVVtTXIlcDuDU0Bvrqofz1XxkqTZ22/wV9Xr99B88T7Wfzfw7j5FSZLmjt/claTGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Jj9Bn+SS5LsSnLbUNvyJJuS3N09Luvak+RDSbYluSXJCXNZvCRp9g7kiP+jwCm7ta0Hrquqo4HrunmAVwJHdz/rgA+Pp0xJ0rjsN/ir6gbg4d2a1wAbu+mNwOlD7ZfWwI3A0iSHj6tYSVJ/o57jX1FVO7vpB4EV3fQRwAND623v2n5KknVJNifZPDMzM2IZkqTZ6n1xt6oKqBG221BV01U1PTU11bcMSdIBGjX4H3r8FE73uKtr3wEcObTeyq5NkrRAjBr81wBru+m1wNVD7W/sPt1zEvDtoVNCkqQFYMn+VkhyOfAy4LAk24HzgQuAK5OcDdwPvK5b/VrgVGAb8APgTXNQsySph/0Gf1W9fi+LTt7DugW8uW9RkqS54zd3JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxvYI/yV8k2ZrktiSXJ3laktVJbkqyLcnHkhw8rmIlSf2NHPxJjgD+HJiuqhcABwFnABcC76+q5wGPAGePo1BJ0nj0PdWzBHh6kiXAM4CdwMuBq7rlG4HTe+5DkjRGIwd/Ve0A3gd8nUHgfxvYAnyrqh7rVtsOHNG3SEnS+PQ51bMMWAOsBp4DHAqcMovt1yXZnGTzzMzMqGVIkmapz6me3wTuraqZqvoR8AngxcDS7tQPwEpgx542rqoNVTVdVdNTU1M9ypAkzUaf4P86cFKSZyQJcDJwO3A98NpunbXA1f1KlCSNU59z/DcxuIh7M3Br91wbgHOBtybZBjwbuHgMdUqSxmTJ/lfZu6o6Hzh/t+Z7gBP7PK8kae74zV1JaozBL0mNMfglqTG9zvFLWnxWrf/UvO37vgtOm7d9t8QjfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhrj/filBWo+74uvxc0jfklqjEf8kprX2l8d84hfkhpj8EtSYwx+SWqMwS9JjekV/EmWJrkqyVeT3JHk15IsT7Ipyd3d47JxFStJ6q/vEf8Hgc9U1S8DxwN3AOuB66rqaOC6bl6StECMHPxJfh54KXAxQFU9WlXfAtYAG7vVNgKn9y1SkjQ+fY74VwMzwEeSfCnJRUkOBVZU1c5unQeBFXvaOMm6JJuTbJ6ZmelRhiRpNvoE/xLgBODDVfVC4PvsdlqnqgqoPW1cVRuqarqqpqempnqUIUmajT7Bvx3YXlU3dfNXMfiP4KEkhwN0j7v6lShJGqeRg7+qHgQeSPJLXdPJwO3ANcDarm0tcHWvCiVJY9X3Xj1/BlyW5GDgHuBNDP4zuTLJ2cD9wOt67kOSNEa9gr+qvgxM72HRyX2eV1KbvBX1z4bf3JWkxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUmL736mnafH29/L4LTpuX/UpaHDzil6TGGPyS1BiDX5IaY/BLUmO8uDuBWrxnuRe0pfHxiF+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhrTO/iTHJTkS0n+tZtfneSmJNuSfCzJwf3LlCSNyziO+M8B7hiavxB4f1U9D3gEOHsM+5AkjUmv4E+yEjgNuKibD/By4KpulY3A6X32IUkar75H/B8A3gH8pJt/NvCtqnqsm98OHNFzH5KkMRo5+JO8CthVVVtG3H5dks1JNs/MzIxahiRplvoc8b8YeHWS+4ArGJzi+SCwNMnjt3teCezY08ZVtaGqpqtqempqqkcZkqTZGDn4q+q8qlpZVauAM4D/qKozgeuB13arrQWu7l2lJGls5uJz/OcCb02yjcE5/4vnYB+SpBGN5S9wVdXngM910/cAJ47jeSVJ4+c3dyWpMQa/JDVm4v/Yeot/eFyS+vCIX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGjBz8SY5Mcn2S25NsTXJO1748yaYkd3ePy8ZXriSprz5H/I8Bb6uqY4CTgDcnOQZYD1xXVUcD13XzkqQFYuTgr6qdVXVzN/1d4A7gCGANsLFbbSNwet8iJUnjM5Zz/ElWAS8EbgJWVNXObtGDwIq9bLMuyeYkm2dmZsZRhiTpAPQO/iTPBD4OvKWqvjO8rKoKqD1tV1Ubqmq6qqanpqb6liFJOkBL+myc5KkMQv+yqvpE1/xQksOrameSw4FdfYuUVq3/1HyXIC0afT7VE+Bi4I6q+ruhRdcAa7vptcDVo5cnSRq3Pkf8Lwb+ALg1yZe7tr8CLgCuTHI2cD/wun4lSpLGaeTgr6r/BLKXxSeP+rySpLnlN3clqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTFzFvxJTklyZ5JtSdbP1X4kSbMzJ8Gf5CDgH4BXAscAr09yzFzsS5I0O3N1xH8isK2q7qmqR4ErgDVztC9J0izMVfAfATwwNL+9a5MkzbMl87XjJOuAdd3s95LcOcLTHAZ8Y3xVLQiLrU+LrT+w+Pq02PoDE9SnXHhAq+2tP784yj7nKvh3AEcOza/s2v5fVW0ANvTZSZLNVTXd5zkWmsXWp8XWH1h8fVps/YHF16dx92euTvV8ETg6yeokBwNnANfM0b4kSbMwJ0f8VfVYkj8FPgscBFxSVVvnYl+SpNmZs3P8VXUtcO1cPX+n16miBWqx9Wmx9QcWX58WW39g8fVprP1JVY3z+SRJC5y3bJCkxiy44E9ySZJdSW4bajs+yX8nuTXJvyR51tCy87rbQtyZ5BVD7QvilhGz6U+S30qypWvfkuTlQ9v8Ste+LcmHkmQ++tPVMqsx6pYfleR7Sd4+1DZxY9QtO65btrVb/rSufSLHKMlTk2zs2u9Ict7QNgtljI5Mcn2S27vX/ZyufXmSTUnu7h6Xde3pxmBbkluSnDD0XGu79e9OsnZC+nNm149bk3w+yfFDzzX7MaqqBfUDvBQ4AbhtqO2LwK9302cBf9tNHwN8BTgEWA18jcHF5IO66ecCB3frHDMB/Xkh8Jxu+gXAjqFtvgCcBAT4NPDKSRijoeVXAf8MvL2bn9QxWgLcAhzfzT8bOGiSxwh4A3BFN/0M4D5g1QIbo8OBE7rpnwPu6n7/3wOs79rXAxd206d2Y5BuTG7q2pcD93SPy7rpZRPQnxc9XieDW+E83p+RxmjBHfFX1Q3Aw7s1Px+4oZveBLymm17D4B/sD6vqXmAbg9tFLJhbRsymP1X1par6n659K/D0JIckORx4VlXdWIPRvhQ4fe6r37NZjhFJTgfuZdCnx03kGAG/DdxSVV/ptv1mVf14wseogEOTLAGeDjwKfIeFNUY7q+rmbvq7wB0M7gawBtjYrbaRJ17zNcClNXAjsLQbo1cAm6rq4ap6hMHrcMrPsCvA7PtTVZ/v6gW4kcF3o2DEMVpwwb8XW3miM7/HE18O29utIRb6LSP21p9hrwFurqofMqh9+9CyhdYf2EufkjwTOBf4m93Wn9Qxej5QST6b5OYk7+jaJ3aMGLwb+z6wE/g68L6qepgFOkZJVjF4d3wTsKKqdnaLHgRWdNMTkw0H2J9hZzN4NwMj9mdSgv8s4E+SbGHwtujRea6nr332J8mxwIXAH81DbaPaW5/eBby/qr43X4WNaG/9WQK8BDize/ydJCfPT4mztrc+nQj8GHgOg1Omb0vy3Pkpcd+6A4mPA2+pqu8ML+veaU3UxxRn258kv8Eg+M/ts995u1fPbFTVVxm8xSbJ84HTukX7ujXEPm8ZMZ/20R+SrAQ+Cbyxqr7WNe/gibd2sMD6A/vs068Cr03yHmAp8JMk/wtsYTLHaDtwQ1V9o1t2LYNz6f/E5I7RG4DPVNWPgF1J/guYZnAkuWDGKMlTGYTkZVX1ia75oSSHV9XO7lTOrq59b9mwA3jZbu2fm8u692aW/SHJccBFDK4dfbNr3u/tcfboZ31R4wAvfKziyRelfqF7fAqDc6dndfPH8uSLu/cwuNixpJtezRMXPI6dgP4s7Wr93T08x+4XDk+dhDHabZt38cTF3Ukdo2XAzQwugi4B/h04bZLHiMHR40e66UOB24HjFtIYda/ppcAHdmt/L0++GPqebvo0nnxx9wtd+3IG15uWdT/3AssnoD9HMbiG+aLd1h9pjObtH+U+XpDLGZxr/BGDo6uzgXMYXPW+C7iA7otn3frvZHBV+06GPkXB4Kr+Xd2yd05Cf4C/ZnCu9ctDP4//sk4Dt3X9+fvh12Ah92m37d5FF/yTOkbd+r/P4Hz5bY//Yk7yGAHPZPCJq60MQv8vF+AYvYTBaY9bhn43TmXwqarrgLsZ/Ce8vFs/DP4Y1NeAW4Hpoec6i0GIbgPeNCH9uQh4ZGjdzX3GyG/uSlJjJuXiriRpTAx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5Ia838FC+oaVWmcqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just a quick check on speech frequency. There is a low number of speeches in the 50s for some reason,\n",
    "# and a spike in the 60s (TV became more widespread and the Civil Rights Movement and Vietnam were major issues)\n",
    "\n",
    "# After the 60s, there were significantly more speeches recorded than previously\n",
    "\n",
    "plt.hist(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LdaModel\n",
    "\n",
    "ldamodel = LdaModel(corpus = corpus, num_topics=20, id2word = dictionary, \n",
    "                                           passes=50, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign doc_topics\n",
    "\n",
    "doc_topics = list(ldamodel.get_document_topics(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.016*\"war\" + 0.011*\"vietnam\" + 0.009*\"peace\" + 0.009*\"south\" + 0.007*\"time\"',\n",
       " '0.018*\"tax\" + 0.007*\"unemployment\" + 0.007*\"ford\" + 0.006*\"investment\" + 0.005*\"nixon\"',\n",
       " '0.007*\"world\" + 0.006*\"president\" + 0.005*\"country\" + 0.005*\"soviet\" + 0.004*\"peace\"',\n",
       " '0.005*\"world\" + 0.005*\"khrushchev\" + 0.005*\"new\" + 0.004*\"freedom\" + 0.004*\"future\"',\n",
       " '0.008*\"president\" + 0.007*\"soviet\" + 0.005*\"gorbachev\" + 0.005*\"peace\" + 0.005*\"answer\"',\n",
       " '0.005*\"country\" + 0.005*\"law\" + 0.004*\"congress\" + 0.004*\"war\" + 0.004*\"national\"',\n",
       " '0.027*\"energy\" + 0.025*\"oil\" + 0.008*\"cuba\" + 0.007*\"schedule\" + 0.006*\"tariff\"',\n",
       " '0.000*\"time\" + 0.000*\"new\" + 0.000*\"country\" + 0.000*\"world\" + 0.000*\"war\"',\n",
       " '0.036*\"panama\" + 0.014*\"canal\" + 0.013*\"isthmus\" + 0.011*\"republic\" + 0.010*\"colon\"',\n",
       " '0.016*\"health\" + 0.016*\"care\" + 0.010*\"insurance\" + 0.006*\"system\" + 0.006*\"plan\"',\n",
       " '0.014*\"congress\" + 0.006*\"country\" + 0.006*\"tax\" + 0.006*\"bill\" + 0.006*\"program\"',\n",
       " '0.008*\"system\" + 0.008*\"business\" + 0.007*\"economic\" + 0.006*\"industry\" + 0.006*\"banks\"',\n",
       " '0.011*\"world\" + 0.009*\"iraq\" + 0.007*\"security\" + 0.007*\"war\" + 0.006*\"peace\"',\n",
       " '0.008*\"department\" + 0.008*\"congress\" + 0.006*\"law\" + 0.005*\"court\" + 0.004*\"foreign\"',\n",
       " '0.007*\"senator\" + 0.007*\"president\" + 0.007*\"right\" + 0.007*\"congress\" + 0.006*\"well\"',\n",
       " '0.013*\"rights\" + 0.009*\"right\" + 0.009*\"law\" + 0.006*\"life\" + 0.006*\"justice\"',\n",
       " '0.007*\"time\" + 0.006*\"well\" + 0.005*\"said\" + 0.005*\"poland\" + 0.005*\"percent\"',\n",
       " '0.020*\"world\" + 0.014*\"peace\" + 0.012*\"nations\" + 0.009*\"new\" + 0.008*\"freedom\"',\n",
       " '0.010*\"lebanon\" + 0.006*\"well\" + 0.005*\"cuba\" + 0.005*\"peace\" + 0.005*\"force\"',\n",
       " '0.008*\"new\" + 0.007*\"work\" + 0.006*\"time\" + 0.005*\"country\" + 0.005*\"world\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine topic clusters\n",
    "\n",
    "[x[1] for x in ldamodel.show_topics(num_topics = 20, num_words = 5)] # remove index to show all topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda topic dataframe - create function from this to take number of topics and keywords\n",
    "\n",
    "lda_results = [x[1] for x in ldamodel.show_topics(num_topics = 20, num_words = 5)]\n",
    "\n",
    "lst = re.findall(r'([a-zA-Z]+)', str(lda_results))\n",
    "\n",
    "lda_topic_list = [lst[x:x+5] for x in range(0, len(lst),5)]\n",
    "\n",
    "lda_df = pd.DataFrame(lda_topic_list)\n",
    "\n",
    "lda_df.index = [f'Topic {topic}' for topic in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA DF FOR GRAPHING TOPICS\n",
    "\n",
    "# assign doc_topics to dataframe (list of list of tuples with percentage of each topic in each doc)\n",
    "\n",
    "df['doc_topic'] = doc_topics\n",
    "\n",
    "# convert to dictionary to easily access values\n",
    "\n",
    "df['doc_topic'] = df['doc_topic'].map(lambda x: dict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NMF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization in sklearn for comparison to LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to get total word counts in documents\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of strings from list of lists for CountVectorizer\n",
    "\n",
    "punc_joined = [' '.join(text) for text in punc_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform words with TfidfTransformer - This takes into account term frequency across and within documents\n",
    "\n",
    "word_counts = vectorizer.fit_transform(punc_joined)\n",
    "\n",
    "tfidf_transform = TfidfTransformer(smooth_idf = False)\n",
    "\n",
    "words_tfidf = tfidf_transform.fit_transform(word_counts)\n",
    "\n",
    "# final_words = normalize(words_tfidf, norm = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate NMF model and fit to tfidf transformed documents\n",
    "\n",
    "model = NMF(n_components = 20, init = 'nndsvd')\n",
    "\n",
    "# Set W as the document by topic matrix\n",
    "# Set H as the topic by word matrix\n",
    "\n",
    "W = model.fit_transform(words_tfidf)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((585, 20), (20, 10000))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check matrix shapes\n",
    "\n",
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign topic indices back to feature names - takes model, feature names from vectorizer, \n",
    "# and n_top_words as arguments. n_top_words selects the number of keywords per topic\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    lst = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        lst.append(message)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 3rd argument to number of topic keywords that are desired\n",
    "\n",
    "topics_nmf = (print_top_words(model, vectorizer.get_feature_names(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic #0: world nations peace new war',\n",
       " 'Topic #1: law congress department interstate commerce',\n",
       " 'Topic #2: health care jobs work new',\n",
       " 'Topic #3: vietnam south vietnamese peace north',\n",
       " 'Topic #4: iraq iraqi saddam iraqis kuwait',\n",
       " 'Topic #5: soviet nuclear missiles weapons union',\n",
       " 'Topic #6: president country got party republican',\n",
       " 'Topic #7: war japanese fighting forces japan',\n",
       " 'Topic #8: banks business credit industry employment',\n",
       " 'Topic #9: viet nam south aggression ambassador',\n",
       " 'Topic #10: tax budget inflation percent billion',\n",
       " 'Topic #11: energy oil congress gas inflation',\n",
       " 'Topic #12: german imperial vessels peace neutral',\n",
       " 'Topic #13: tariff cuba bill revision duties',\n",
       " 'Topic #14: israel lebanon peace israeli palestinian',\n",
       " 'Topic #15: negro law rights right negroes',\n",
       " 'Topic #16: afghanistan qaeda terrorists al terror',\n",
       " 'Topic #17: republic world civilization party nationality',\n",
       " 'Topic #18: freedom god life new day',\n",
       " 'Topic #19: panama canal isthmus republic colon']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nmf topic assignment\n",
    "\n",
    "topics_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from nmf topics and keywords\n",
    "\n",
    "nmfsplit = [item.split() for item in topics_nmf]\n",
    "\n",
    "nmf_df = pd.DataFrame(nmfsplit)\n",
    "\n",
    "nmf_df.index = [f'Topic {topic}' for topic in range(20)]\n",
    "\n",
    "nmf_df.drop(columns = [0, 1], inplace = True)\n",
    "\n",
    "nmf_df.columns = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv if desired\n",
    "\n",
    "# nmf_df.to_csv('nmf_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy corpus df\n",
    "\n",
    "corpus_df = df[df.columns[0:4]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>pres_name</th>\n",
       "      <th>speech</th>\n",
       "      <th>doc_topic</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-09-25</td>\n",
       "      <td>Debate with Michael Dukakis</td>\n",
       "      <td>bush</td>\n",
       "      <td>I think we've seen a deterioration of ...</td>\n",
       "      <td>{2: 0.3027503, 10: 0.04934066, 19: 0.64316326}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.010888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989-01-20</td>\n",
       "      <td>Inaugural Address</td>\n",
       "      <td>bush</td>\n",
       "      <td>Mr. Chief Justice, Mr. President, Vice Presi...</td>\n",
       "      <td>{2: 0.039600223, 9: 0.08852721, 17: 0.21697919...</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-08-18</td>\n",
       "      <td>Acceptance Speech at the Republican National C...</td>\n",
       "      <td>bush</td>\n",
       "      <td>I have many friends to thank tonight. I than...</td>\n",
       "      <td>{17: 0.07929415, 18: 0.12963656, 19: 0.7887595}</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.029660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171989</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989-12-20</td>\n",
       "      <td>Address to the Nation on Panama</td>\n",
       "      <td>bush</td>\n",
       "      <td>My fellow citizens, last night I ordered U. ...</td>\n",
       "      <td>{0: 0.07677447, 2: 0.4519084, 8: 0.123605825, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.489844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989-05-12</td>\n",
       "      <td>Commencement Address at Texas A and M University</td>\n",
       "      <td>bush</td>\n",
       "      <td>Thank you, Governor. Thank you all very much...</td>\n",
       "      <td>{2: 0.07152886, 9: 0.2906991, 17: 0.17870894, ...</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                              title pres_name  \\\n",
       "0 1988-09-25                        Debate with Michael Dukakis      bush   \n",
       "1 1989-01-20                                  Inaugural Address      bush   \n",
       "2 1988-08-18  Acceptance Speech at the Republican National C...      bush   \n",
       "3 1989-12-20                    Address to the Nation on Panama      bush   \n",
       "4 1989-05-12   Commencement Address at Texas A and M University      bush   \n",
       "\n",
       "                                              speech  \\\n",
       "0          I think we've seen a deterioration of ...   \n",
       "1    Mr. Chief Justice, Mr. President, Vice Presi...   \n",
       "2    I have many friends to thank tonight. I than...   \n",
       "3    My fellow citizens, last night I ordered U. ...   \n",
       "4    Thank you, Governor. Thank you all very much...   \n",
       "\n",
       "                                           doc_topic   topic_0   topic_1  \\\n",
       "0     {2: 0.3027503, 10: 0.04934066, 19: 0.64316326}  0.000000  0.000000   \n",
       "1  {2: 0.039600223, 9: 0.08852721, 17: 0.21697919...  0.024343  0.001671   \n",
       "2    {17: 0.07929415, 18: 0.12963656, 19: 0.7887595}  0.000600  0.000000   \n",
       "3  {0: 0.07677447, 2: 0.4519084, 8: 0.123605825, ...  0.000000  0.000000   \n",
       "4  {2: 0.07152886, 9: 0.2906991, 17: 0.17870894, ...  0.008966  0.000279   \n",
       "\n",
       "    topic_2   topic_3   topic_4    ...     topic_10  topic_11  topic_12  \\\n",
       "0  0.078473  0.000000  0.006845    ...     0.040244  0.000000  0.006703   \n",
       "1  0.047372  0.025377  0.000000    ...     0.000000  0.006983  0.000000   \n",
       "2  0.104078  0.000000  0.011462    ...     0.011950  0.029660  0.000000   \n",
       "3  0.000000  0.000000  0.005868    ...     0.000000  0.000000  0.000000   \n",
       "4  0.018498  0.002416  0.000000    ...     0.000000  0.000000  0.003577   \n",
       "\n",
       "   topic_13  topic_14  topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "0       0.0  0.000000       0.0  0.000664  0.000000  0.002572  0.010888  \n",
       "1       0.0  0.000000       0.0  0.000000  0.000000  0.244156  0.000000  \n",
       "2       0.0  0.011141       0.0  0.000940  0.000000  0.171989  0.000000  \n",
       "3       0.0  0.000000       0.0  0.026344  0.000000  0.007533  0.489844  \n",
       "4       0.0  0.001893       0.0  0.003551  0.002375  0.052883  0.000967  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create NMF DataFrame for graphing\n",
    "\n",
    "lsi_df = pd.DataFrame(W)\n",
    "\n",
    "lsi_df.columns = [\"topic_%d\" % col for col in lsi_df.columns]\n",
    "\n",
    "lsi_df = pd.concat([corpus_df.reset_index(), lsi_df], axis=1)\n",
    "\n",
    "lsi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the largest topic value from NMF results\n",
    "\n",
    "lsi_df.index = lsi_df['index']\n",
    "\n",
    "lsi_df = lsi_df.drop(columns = ['index'])\n",
    "\n",
    "lsi_df.index = pd.to_datetime(lsi_df.index)\n",
    "\n",
    "lsi_df['max_topic'] = lsi_df.loc[:, 'topic_0' : 'topic_19'].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf topic sum - these will not sum to 1, since they are not percentage of doc based like LDA topics\n",
    "\n",
    "# lsi_df['sum_topic'] = lsi_df.loc[:, 'topic_0' : 'topic_19'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Topics over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing function for NMF results - can uncomment color and label to add arguments to graphing function\n",
    "\n",
    "def graph_lsi(topic_num): #, color, label):\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.title(f'Topic Number {topic_num}')\n",
    "    plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "    plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2018-01-01'))\n",
    "    plt.ylim(0, .5)\n",
    "    plt.vlines(x = lsi_df.index, ymin = 0, ymax = lsi_df[f'topic_{topic_num}'], alpha = .3, )#color = color, label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing function for LDA results\n",
    "\n",
    "def graph_topic(topic_num):\n",
    "    \n",
    "    df[f'topic_{topic_num}'] = df['doc_topic'].apply(lambda x: x.get(topic_num, 0))\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.title(f'Topic Number {topic_num}')\n",
    "    plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "    plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2018-01-01'))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.vlines(x = df.index, ymin = 0, ymax = df[f'topic_{topic_num}'], alpha = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(20):\n",
    "#    graph_topic(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NMF Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_lsi(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['yellow', 'orange', 'green', 'blue', 'yellow', 'red']\n",
    "labels = ['Vietnam', 'Iraq + Afghanistan', 'Cold War', 'WW2', 'Vietnam', 'WW1']\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.title('20th Century US Wars Modeled with NMF', fontsize = 20)\n",
    "\n",
    "for color, topic in enumerate([3, 4, 5, 7, 9, 18]):\n",
    "    graph_lsi(topic, colors[color], labels[color])\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = cm.get_cmap(name='hsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [str(i)]) for i, doc in enumerate(punc_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model = Doc2Vec(documents = documents, vector_size=500,\n",
    "                                      window=10, min_count=5, workers=2, seed=42, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [doc for doc in docvec_model.docvecs.vectors_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(num_topics = 20, num_words = 20) # remove index to show all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docvec_model.wv.most_similar('economy', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model.wv.most_similar('iraq', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model.wv.most_similar('energy', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docvec_model.docvecs.doctags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering with Vectors from Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 20\n",
    "\n",
    "centroids, _ = kmeans(vecs, NUMBER_OF_CLUSTERS)\n",
    "\n",
    "# computes cluster Id for document vectors\n",
    "doc_ids, _ = vq(vecs, centroids)\n",
    "\n",
    "# zips cluster Ids back to document labels \n",
    "doc_labels = zip(docvec_model.docvecs.doctags.keys(), doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = list(doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model.fit(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = kmeans_model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tags = zip(docvec_model.docvecs.doctags.keys(), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zlist = list(kmeans_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, zlist = zip(*zlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [int(item) for item in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_z = np.reshape(z, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_zlist = np.reshape(zlist, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_score(sil_z, zlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(z, zlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = 8, min_samples = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [doc for doc in docvec_model.docvecs.vectors_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 20\n",
    "\n",
    "centroids, _ = kmeans(vecs, NUMBER_OF_CLUSTERS)\n",
    "\n",
    "# computes cluster Id for document vectors\n",
    "doc_ids,_ = vq(vecs,centroids)\n",
    "\n",
    "# zips cluster Ids back to document labels \n",
    "doc_labels = zip(docvec_model.docvecs.doctags.keys(), doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 20\n",
    "\n",
    "centroids, _ = kmeans(docvec_model.docvecs, NUMBER_OF_CLUSTERS)\n",
    "\n",
    "# computes cluster Id for document vectors\n",
    "doc_ids,_ = vq(docvec_model.docvecs,centroids)\n",
    "\n",
    "# zips cluster Ids back to document labels \n",
    "doc_labels = zip(model.docvecs.doctags.keys(), doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model.most_similar('vietnam', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model.wv.distances('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_model.vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(size=100, dbow_words= 1, dm=0, iter=1,  window=5, seed=1337, min_count=2, workers=4,alpha=0.025, min_alpha=0.025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus = corpus, num_topics=10, id2word = dictionary, \n",
    "                                           passes=100, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punc_remove[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped_titles = list(zip(titles, doc_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_topics = [(0, 'immigration'), (1, 'cold_war'), (2, 'legislation'), (3, '?'), (4, 'gov_powers'), \n",
    "#                   (5, 'banking_system'), (6, 'capital_markets'), (7, ''), (),\n",
    "#                  (), (), (), (),\n",
    "#                  (), (), (), (),\n",
    "#                  (), (), (), (),\n",
    "#                  (), (), (), (),\n",
    "#                  ()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(num_topics = 30, num_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_topic'] = doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_topic'] = df['doc_topic'].map(lambda x: dict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pres_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_string(topic_num):\n",
    "    \n",
    "    df[f'topic_{topic_num}'] = df['doc_topic'].apply(lambda x: x.get(topic_num, 0))\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.title(f'Topic Number {topic_num}')\n",
    "    plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "    plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.vlines(x = df.index, ymin = 0, ymax = df[f'topic_{topic_num}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_string(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_0'] = df['doc_topic'].apply(lambda x: x.get(0, 0))\n",
    "\n",
    "df_0 = df[df['topic_0'] != 0]\n",
    "\n",
    "df_0.index = pd.to_datetime(df_0.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.title('Frequency of Topics Related to the Panama Canal', fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "plt.ylim(0, 1)\n",
    "plt.vlines(x = df_0.index, ymin = 0, ymax = df_0['topic_0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_12'] = df['doc_topic'].apply(lambda x: x.get(12, 0))\n",
    "\n",
    "df_12 = df[df['topic_12'] != 0]\n",
    "\n",
    "df_12.index = pd.to_datetime(df_12.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.title('War Iraq and Germany', fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "plt.ylim(0, 1)\n",
    "plt.vlines(x = df_12.index, ymin = 0, ymax = df_12['topic_12'], alpha = .3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_4'] = df['doc_topic'].apply(lambda x: x.get(4, 0))\n",
    "\n",
    "df_4 = df[df['topic_4'] != 0]\n",
    "\n",
    "df_4.index = pd.to_datetime(df_4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.title('Energy Oil Inflation', fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "plt.ylim(0, 1)\n",
    "plt.vlines(x = df_4.index, ymin = 0, ymax = df_4['topic_4'], alpha = .3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_4'] = df['doc_topic'].apply(lambda x: x.get(4, 0))\n",
    "\n",
    "df_4 = df[df['topic_4'] != 0]\n",
    "\n",
    "df_4.index = pd.to_datetime(df_4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.title('Energy Oil Inflation', fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "plt.ylim(0, 1)\n",
    "plt.vlines(x = df_4.index, ymin = 0, ymax = df_4['topic_4'], alpha = .3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_string(topic_num):\n",
    "    df[f'topic_{topic_num}'] = df['doc_topic'].apply(lambda x: x.get(topic_num, 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_plot(topic_num, topic_label):\n",
    "    df[f'topic_{topic_num}'] = df['doc_topic'].apply(lambda x: x.get(topic_num, 0))\n",
    "\n",
    "    topic_label = df[df[f'topic_{topic_num}'] != 0]\n",
    "\n",
    "    topic_label.index = pd.to_datetime(topic_label.index)\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.title(f'{topic_label}', fontsize = 20)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Percentage of Speech on Topic', fontsize = 16)\n",
    "    plt.xlim(pd.Timestamp('1900-01-01'), pd.Timestamp('2020-01-01'))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.vlines(x = topic_label.index, ymin = 0, ymax = topic_label[f'topic_{topic_num}']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_plot('12', 'wars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.state.get_lambda()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title' : titles, 'doc_topic' : doc_topics, 'pres_name' : pres_names2}, index = speech_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'] == 'Remarks at the Manned Space Flight Center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_feed = [item for sublist in punc_remove for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = vectorizer.fit_transform(vectorizer_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_input = [' '.join(x) for x in punc_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_input = np.reshape(lda_input, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.fit_transform(lda_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = [item for sublist in punc_remove for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in filtering for punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prac_sent = \"this is a short sentence, that's got some punctuation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prac_sent.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obama_sents[4].translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_sents_tokenized = [nltk.word_tokenize(word) for word in punc_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_sents_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all parts of speech\n",
    "\n",
    "# [nltk.pos_tag(obama_sents_tokenized[index]) for index, x in enumerate(obama_sents_tokenized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT DO THIS\n",
    "# [get_wordnet_pos(word) for index, word in enumerate(sentence) for sentence in obama_sents_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(obama_sents_tokenized[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_sents_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
